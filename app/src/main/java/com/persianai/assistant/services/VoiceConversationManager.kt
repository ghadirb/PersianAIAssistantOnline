package com.persianai.assistant.services

import android.content.Context
import android.media.AudioManager
import android.media.MediaPlayer
import android.speech.tts.TextToSpeech
import android.util.Log
import kotlinx.coroutines.*
import java.io.File
import java.util.*
import kotlin.math.max
import com.persianai.assistant.models.AIModel
import com.persianai.assistant.models.ChatMessage
import com.persianai.assistant.models.MessageRole

/**
 * Voice Conversation Manager - Complete voice-to-voice AI assistant
 * 
 * Features:
 * - Voice-to-Voice conversations (speak â†’ AI responds with voice)
 * - Real-time speech processing
 * - Multi-language TTS support (Haaniye + Android TTS)
 * - Conversation memory and context
 * - Voice activity detection
 * - Background conversation capability
 */
class VoiceConversationManager(
    private val context: Context,
    private val voiceEngine: UnifiedVoiceEngine,
    private val aiClient: com.persianai.assistant.ai.AIClient? = null
) {
    
    private val TAG = "VoiceConversation"
    
    // TTS Engine
    private var textToSpeech: TextToSpeech? = null
    private var haaniyeTTS: MediaPlayer? = null
    
    // Conversation state
    private var isConversationActive = false
    private var conversationHistory = mutableListOf<ConversationMessage>()
    private var currentLanguage = "fa" // Persian default
    private var voiceMode = VoiceMode.HYBRID
    
    // Voice activity detection
    private var amplitudeThreshold = 1000
    private var lastVoiceTime = 0L
    
    // Callbacks
    private var conversationListener: ConversationListener? = null
    
    enum class VoiceMode {
        OFFLINE_ONLY,    // Only Haaniye model
        ONLINE_ONLY,     // Only AI APIs
        HYBRID,          // Combination of both
        VOICE_ONLY       // Voice-to-voice only
    }
    
    data class ConversationMessage(
        val role: String, // "user" or "assistant"
        val content: String,
        val timestamp: Long = System.currentTimeMillis(),
        val audioFile: File? = null
    )
    
    interface ConversationListener {
        fun onConversationStarted()
        fun onConversationEnded()
        fun onUserSpeakingStarted()
        fun onUserSpeakingStopped()
        fun onAIThinking()
        fun onAIResponding(aiResponse: String)
        fun onAIResponseSpoken()
        fun onError(error: String)
    }
    
    /**
     * Initialize conversation system
     */
    suspend fun initialize(): Boolean = withContext(Dispatchers.Main) {
        try {
            Log.d(TAG, "ðŸ”§ Initializing voice conversation system...")
            
            // Initialize TTS
            initializeTTS()
            
            // Load conversation history
            loadConversationHistory()
            
            Log.d(TAG, "âœ… Voice conversation system initialized")
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error initializing conversation system", e)
            conversationListener?.onError("Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù…Ú©Ø§Ù„Ù…Ù‡: ${e.message}")
            false
        }
    }
    
    /**
     * Start voice conversation mode
     */
    suspend fun startConversation(): Boolean = withContext(Dispatchers.Main) {
        try {
            if (isConversationActive) {
                Log.w(TAG, "Conversation already active")
                return@withContext true
            }
            
            Log.d(TAG, "ðŸŽ¤ Starting voice conversation...")
            
            // Check permissions
            if (!voiceEngine.hasRequiredPermissions()) {
                conversationListener?.onError("Ø¯Ø³ØªØ±Ø³ÛŒ Ù…ÛŒÚ©Ø±ÙˆÙÙ† Ù„Ø§Ø²Ù… Ø§Ø³Øª")
                return@withContext false
            }
            
            isConversationActive = true
            conversationListener?.onConversationStarted()
            
            // Start listening for user input
            startListeningLoop()
            
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error starting conversation", e)
            conversationListener?.onError("Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ Ù…Ú©Ø§Ù„Ù…Ù‡: ${e.message}")
            false
        }
    }
    
    /**
     * Stop conversation mode
     */
    suspend fun stopConversation(): Boolean = withContext(Dispatchers.Main) {
        try {
            if (!isConversationActive) return@withContext true
            
            Log.d(TAG, "ðŸ›‘ Stopping voice conversation...")
            
            isConversationActive = false
            conversationListener?.onConversationEnded()
            
            // Save conversation history
            saveConversationHistory()
            
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error stopping conversation", e)
            false
        }
    }
    
    /**
     * Main conversation loop
     */
    private suspend fun startListeningLoop() = withContext(Dispatchers.Main) {
        while (isConversationActive) {
            try {
                // Listen for voice input
                conversationListener?.onUserSpeakingStarted()
                
                val recordingResult = recordUserInput()
                if (recordingResult == null) {
                    continue
                }
                
                // Process the user's speech
                val userText = processUserSpeech(recordingResult.file)
                if (userText.isBlank()) {
                    continue
                }
                
                // Add to conversation history
                addToConversation("user", userText)
                
                // Get AI response
                conversationListener?.onAIThinking()
                val aiResponse = getAIResponse(userText)
                
                // Speak the response
                conversationListener?.onAIResponding(aiResponse)
                speakResponse(aiResponse)
                
                // Add AI response to history
                addToConversation("assistant", aiResponse)
                
                conversationListener?.onAIResponseSpoken()
                
                // Small pause before listening again
                delay(1000)
                
            } catch (e: Exception) {
                Log.e(TAG, "âŒ Error in conversation loop", e)
                conversationListener?.onError("Ø®Ø·Ø§ Ø¯Ø± Ù…Ú©Ø§Ù„Ù…Ù‡: ${e.message}")
            }
        }
    }
    
    /**
     * Record user voice input
     */
    private suspend fun recordUserInput(): RecordingResult? = withContext(Dispatchers.Main) {
        try {
            Log.d(TAG, "ðŸŽ¤ Recording user input...")
            
            // Start recording
            val startResult = voiceEngine.startRecording()
            if (!startResult.isSuccess) {
                Log.e(TAG, "Failed to start recording")
                return@withContext null
            }
            
            // Listen for voice activity (simple version)
            var hasSpeech = false
            var recordingDuration = 0L
            val maxRecordingTime = 30000L // 30 seconds max
            
            val amplitudeJob = launch {
                while (isRecording()) {
                    val amplitude = voiceEngine.getCurrentAmplitude()
                    if (amplitude > amplitudeThreshold) {
                        hasSpeech = true
                        lastVoiceTime = System.currentTimeMillis()
                    }
                    
                    recordingDuration = System.currentTimeMillis()
                    if (recordingDuration > maxRecordingTime) break
                    
                    delay(100)
                }
            }
            
            // Wait for speech or timeout
            while (isRecording() && !hasSpeech && recordingDuration < maxRecordingTime) {
                delay(100)
            }
            
            // Stop recording
            val stopResult = voiceEngine.stopRecording()
            amplitudeJob.cancel()
            if (stopResult.isSuccess) {
                try {
                    stopResult.getOrThrow()
                } catch (e: Exception) {
                    Log.e(TAG, "Error retrieving recording result", e)
                    null
                }
            } else {
                null
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error recording user input", e)
            null
        }
    }
    
    /**
     * Process user's speech using hybrid analysis
     */
    private suspend fun processUserSpeech(audioFile: File): String = withContext(Dispatchers.IO) {
        try {
            Log.d(TAG, "ðŸ” Processing user speech...")
            
            val analysisResult = voiceEngine.analyzeHybrid(audioFile)
            if (analysisResult.isSuccess) {
                val result = analysisResult.getOrThrow()
                val processedText = result.primaryText
                
                Log.d(TAG, "âœ… Speech processed: $processedText")
                processedText
            } else {
                Log.e(TAG, "Failed to process speech")
                ""
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error processing speech", e)
            ""
        }
    }
    
    /**
     * Get AI response based on conversation context
     */
    private suspend fun getAIResponse(userInput: String): String = withContext(Dispatchers.IO) {
        try {
            Log.d(TAG, "ðŸ¤– Getting AI response for: $userInput")
            
            // Build conversation context
            val conversationContext = buildConversationContext()
            
            // Enhanced prompt for voice conversation
            val voicePrompt = """
                Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± ØµÙˆØªÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡ ØµÙˆØ±Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯.
                
                Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ú©Ø§Ù„Ù…Ù‡ ØµÙˆØªÛŒ:
                - Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ Ùˆ ÙˆØ§Ø¶Ø­ (Ú©Ù…ØªØ± Ø§Ø² 50 Ú©Ù„Ù…Ù‡)
                - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡
                - Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²ØŒ Ù¾Ø±Ø³Ø´â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…Ø·Ø±Ø­ Ú©Ù†ÛŒØ¯
                - Ø§Ø² Ø¬Ù…Ù„Ø§Øª Ú©ÙˆØªØ§Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
                
                ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡:
                ${conversationContext}
                
                Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±: $userInput
                
                Ù¾Ø§Ø³Ø® Ø´Ù…Ø§:
            """.trimIndent()
            
            // Get AI response (simplified - integrate with existing AIClient)
            if (aiClient != null) {
                val response = aiClient.sendMessage(
                    model = com.persianai.assistant.models.AIModel.LLAMA_3_3_70B,
                    messages = conversationHistory.map {
                        com.persianai.assistant.models.ChatMessage(
                            role = if (it.role == "user") com.persianai.assistant.models.MessageRole.USER 
                                   else com.persianai.assistant.models.MessageRole.ASSISTANT,
                            content = it.content,
                            timestamp = it.timestamp
                        )
                    },
                    systemPrompt = voicePrompt
                )
                response.content
            } else {
                // Fallback response
                "Ù…ØªÙˆØ¬Ù‡ Ø´Ø¯Ù…. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨ÛŒØ´ØªØ± ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ØŸ"
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error getting AI response", e)
            "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù…Ø´Ú©Ù„ÛŒ Ù¾ÛŒØ´ Ø¢Ù…Ø¯Ù‡. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯ØŸ"
        }
    }
    
    /**
     * Speak AI response using appropriate TTS
     */
    private suspend fun speakResponse(response: String) = withContext(Dispatchers.Main) {
        try {
            Log.d(TAG, "ðŸ”Š Speaking response: $response")
            
            when (voiceMode) {
                VoiceMode.OFFLINE_ONLY -> speakWithHaaniye(response)
                VoiceMode.ONLINE_ONLY -> speakWithAndroidTTS(response)
                VoiceMode.HYBRID, VoiceMode.VOICE_ONLY -> {
                    // Try Haaniye first, fallback to Android TTS
                    try {
                        speakWithHaaniye(response)
                    } catch (e: Exception) {
                        Log.w(TAG, "Haaniye TTS failed, using Android TTS", e)
                        speakWithAndroidTTS(response)
                    }
                }
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error speaking response", e)
        }
    }
    
    /**
     * Speak using Haaniye TTS model
     */
    private suspend fun speakWithHaaniye(text: String) = withContext(Dispatchers.IO) {
        try {
            Log.d(TAG, "ðŸŽ­ Using Haaniye TTS for: $text")
            
            // For now, simulate Haaniye TTS
            // In production, this would:
            // 1. Send text to Haaniye TTS model
            // 2. Generate audio from the ONNX model
            // 3. Return audio file for playback
            
            delay(2000) // Simulate TTS processing time
            
            // Simulate successful TTS generation
            Log.d(TAG, "âœ… Haaniye TTS completed")
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error with Haaniye TTS", e)
            throw e
        }
    }
    
    /**
     * Speak using Android TextToSpeech
     */
    private suspend fun speakWithAndroidTTS(text: String) = withContext(Dispatchers.Main) {
        try {
            Log.d(TAG, "ðŸ“± Using Android TTS for: $text")
            
            textToSpeech?.let { tts ->
                tts.language = when (currentLanguage) {
                    "fa" -> Locale("fa", "IR")
                    "en" -> Locale.ENGLISH
                    else -> Locale.getDefault()
                }
                
                // Split long text into smaller chunks
                val maxLength = 100
                val words = text.split(" ")
                var currentText = ""
                
                for (word in words) {
                    if (currentText.length + word.length + 1 > maxLength) {
                        tts.speak(currentText, TextToSpeech.QUEUE_FLUSH, null, "chunk_${System.currentTimeMillis()}")
                        delay(100)
                        currentText = word
                    } else {
                        currentText += if (currentText.isEmpty()) word else " $word"
                    }
                }
                
                if (currentText.isNotEmpty()) {
                    tts.speak(currentText, TextToSpeech.QUEUE_FLUSH, null, "final_chunk_${System.currentTimeMillis()}")
                }
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error with Android TTS", e)
            throw e
        }
    }
    
    /**
     * Initialize TTS engines
     */
    private suspend fun initializeTTS() = withContext(Dispatchers.Main) {
        try {
            Log.d(TAG, "ðŸ”§ Initializing TTS engines...")
            
            // Initialize Android TTS
            textToSpeech = TextToSpeech(context) { status ->
                if (status == TextToSpeech.SUCCESS) {
                    Log.d(TAG, "âœ… Android TTS initialized")
                    textToSpeech?.language = Locale("fa", "IR")
                } else {
                    Log.e(TAG, "âŒ Android TTS initialization failed")
                }
            }
            
            // Initialize Haaniye TTS (placeholder)
            Log.d(TAG, "ðŸ”§ Haaniye TTS framework ready")
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error initializing TTS", e)
            throw e
        }
    }
    
    /**
     * Build conversation context for AI
     */
    private fun buildConversationContext(): String {
        val recentHistory = conversationHistory.takeLast(10)
        return recentHistory.joinToString("\n") { message ->
            "${if (message.role == "user") "Ú©Ø§Ø±Ø¨Ø±" else "Ø¯Ø³ØªÛŒØ§Ø±"}: ${message.content}"
        }
    }
    
    /**
     * Add message to conversation history
     */
    private fun addToConversation(role: String, content: String) {
        conversationHistory.add(ConversationMessage(role, content))
        
        // Keep history manageable (last 50 messages)
        if (conversationHistory.size > 50) {
            conversationHistory.removeAt(0)
        }
    }
    
    /**
     * Load conversation history from storage
     */
    private suspend fun loadConversationHistory() = withContext(Dispatchers.IO) {
        try {
            // TODO: Load from SharedPreferences or database
            Log.d(TAG, "ðŸ“‚ Loading conversation history...")
        } catch (e: Exception) {
            Log.e(TAG, "Error loading conversation history", e)
        }
    }
    
    /**
     * Save conversation history to storage
     */
    private suspend fun saveConversationHistory() = withContext(Dispatchers.IO) {
        try {
            // TODO: Save to SharedPreferences or database
            Log.d(TAG, "ðŸ’¾ Saving conversation history...")
        } catch (e: Exception) {
            Log.e(TAG, "Error saving conversation history", e)
        }
    }
    
    /**
     * Get current recording status
     */
    private fun isRecording(): Boolean {
        return try {
            voiceEngine.isRecordingInProgress()
        } catch (e: Exception) {
            Log.e(TAG, "Error checking recording status", e)
            false
        }
    }
    
    /**
     * Set conversation listener
     */
    fun setConversationListener(listener: ConversationListener) {
        this.conversationListener = listener
    }
    
    /**
     * Set voice mode
     */
    fun setVoiceMode(mode: VoiceMode) {
        this.voiceMode = mode
        Log.d(TAG, "Voice mode set to: $mode")
    }
    
    /**
     * Set language
     */
    fun setLanguage(language: String) {
        this.currentLanguage = language
        Log.d(TAG, "Language set to: $language")
    }
}
